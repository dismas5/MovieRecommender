{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GBGOoChVFIUZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the data\n",
        "column_names = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
        "data = pd.read_csv('u.data', sep='\\t', names=column_names)\n",
        "\n",
        "# Load movie titles\n",
        "movie_titles = pd.read_csv('u.item', sep='|', header=None, encoding='latin-1')\n",
        "movie_titles = movie_titles[[0, 1]]\n",
        "movie_titles.columns = ['movie_id', 'title']\n",
        "\n",
        "# Merge the dataframes\n",
        "data = pd.merge(data, movie_titles, on='movie_id')\n",
        "\n",
        "# Normalize user and movie IDs\n",
        "data['user_id'] = data['user_id'].astype('category').cat.codes.values\n",
        "data['movie_id'] = data['movie_id'].astype('category').cat.codes.values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train, test = train_test_split(data, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RecommenderNet(tf.keras.Model):\n",
        "    def __init__(self, num_users, num_movies, embedding_size, **kwargs):\n",
        "        super(RecommenderNet, self).__init__(**kwargs)\n",
        "        self.num_users = num_users\n",
        "        self.num_movies = num_movies\n",
        "        self.user_embedding = tf.keras.layers.Embedding(num_users + 1, embedding_size, embeddings_initializer='he_normal')\n",
        "        self.user_bias = tf.keras.layers.Embedding(num_users + 1, 1)\n",
        "        self.movie_embedding = tf.keras.layers.Embedding(num_movies + 1, embedding_size, embeddings_initializer='he_normal')\n",
        "        self.movie_bias = tf.keras.layers.Embedding(num_movies + 1, 1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        user_vector = self.user_embedding(inputs[:, 0])\n",
        "        user_bias = self.user_bias(inputs[:, 0])\n",
        "        movie_vector = self.movie_embedding(inputs[:, 1])\n",
        "        movie_bias = self.movie_bias(inputs[:, 1])\n",
        "\n",
        "        dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)\n",
        "\n",
        "        x = dot_user_movie + user_bias + movie_bias\n",
        "\n",
        "        return tf.nn.sigmoid(x)\n",
        "\n",
        "num_users = len(data['user_id'].unique())\n",
        "num_movies = len(data['movie_id'].unique())\n",
        "embedding_size = 50\n",
        "\n",
        "model = RecommenderNet(num_users, num_movies, embedding_size)\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n"
      ],
      "metadata": {
        "id": "m1jh-uYOFPm2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((train[['user_id', 'movie_id']].values, train['rating'].values))\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test[['user_id', 'movie_id']].values, test['rating'].values))\n",
        "\n",
        "batch_size = 64\n",
        "train_data = train_data.shuffle(len(train)).batch(batch_size)\n",
        "test_data = test_data.batch(batch_size)\n",
        "\n",
        "history = model.fit(train_data, epochs=3, validation_data=test_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzZQAbjUFPiN",
        "outputId": "00575c9c-e5d9-4349-a377-ca0e524dca1c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1250/1250 [==============================] - 17s 13ms/step - loss: -160.7797 - val_loss: -858.7333\n",
            "Epoch 2/3\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: -3228.0466 - val_loss: -6137.4023\n",
            "Epoch 3/3\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: -10212.5391 - val_loss: -14434.9678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_new_user_preferences(data, new_user_preferences, movie_titles):\n",
        "    # Create a DataFrame for new user preferences\n",
        "    new_user_ratings = pd.DataFrame(new_user_preferences)\n",
        "\n",
        "    # Encode the movie titles\n",
        "    movie_to_id = {v: k for k, v in movie_titles[['movie_id', 'title']].to_dict('split')['data']}\n",
        "    new_user_ratings['movie_id'] = new_user_ratings['movie_title'].map(movie_to_id)\n",
        "\n",
        "    # Normalize ratings to match the training data\n",
        "    new_user_ratings['user_id'] = data['user_id'].max() + 1  # Assign a new unique user ID\n",
        "    new_user_ratings['rating'] = new_user_ratings['user_rating'] / 5.0  # Scale ratings to [0, 1]\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    new_user_ratings = new_user_ratings[['user_id', 'movie_id', 'rating']]\n",
        "\n",
        "    # Append to the existing data using concat\n",
        "    augmented_data = pd.concat([data, new_user_ratings], ignore_index=True)\n",
        "    return augmented_data, new_user_ratings['user_id'].iloc[0]\n",
        "\n",
        "# Function to recommend movies\n",
        "def recommend_movies(model, data, new_user_preferences, movie_titles, top_k=10, retrain_epochs=1):\n",
        "    # Add new user preferences to the dataset\n",
        "    augmented_data, new_user_id = add_new_user_preferences(data, new_user_preferences, movie_titles)\n",
        "\n",
        "    # Create datasets\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((augmented_data[['user_id', 'movie_id']].values, augmented_data['rating'].values))\n",
        "    batch_size = 64\n",
        "    train_data = train_data.shuffle(len(augmented_data)).batch(batch_size)\n",
        "\n",
        "    # Retrain the model briefly with the new user preferences\n",
        "    model.fit(train_data, epochs=retrain_epochs, verbose=1)\n",
        "\n",
        "    # Generate predictions for all movies for the new user\n",
        "    all_movies = movie_titles['movie_id'].values\n",
        "    user_movie_array = np.hstack((np.array([new_user_id] * len(all_movies)).reshape(-1, 1), all_movies.reshape(-1, 1)))\n",
        "\n",
        "    predictions = model.predict(user_movie_array).flatten()\n",
        "    top_indices = predictions.argsort()[-top_k:][::-1]\n",
        "\n",
        "    recommended_movie_ids = all_movies[top_indices]\n",
        "    recommended_movies = movie_titles[movie_titles['movie_id'].isin(recommended_movie_ids)]\n",
        "\n",
        "    return recommended_movies['title'].values\n",
        "\n",
        "# Reload the original dataset\n",
        "data = pd.read_csv('u.data', sep='\\t', names=column_names)\n",
        "data = pd.merge(data, movie_titles, on='movie_id')\n",
        "data['user_id'] = data['user_id'].astype('category').cat.codes.values\n",
        "data['movie_id'] = data['movie_id'].astype('category').cat.codes.values\n",
        "data['rating'] = data['rating'] / 5.0  # Normalize ratings\n",
        "\n",
        "# Build and compile the model again to reset it\n",
        "num_users = len(data['user_id'].unique())\n",
        "num_movies = len(data['movie_id'].unique())\n",
        "embedding_size = 50\n",
        "\n",
        "model = RecommenderNet(num_users, num_movies, embedding_size)\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
        "\n",
        "# Train the model on the original data\n",
        "train_data = tf.data.Dataset.from_tensor_slices((data[['user_id', 'movie_id']].values, data['rating'].values))\n",
        "train_data = train_data.shuffle(len(data)).batch(batch_size)\n",
        "model.fit(train_data, epochs=5, validation_data=test_data)\n",
        "\n",
        "# Test the recommendations\n",
        "new_user_preferences_1 = [\n",
        "    {\"movie_title\": \"Lion King, The (1994)\", \"user_rating\": 5.0},\n",
        "    {\"movie_title\": \"Akira (1988)\", \"user_rating\": 5.0},\n",
        "    {\"movie_title\": \"Cinderella (1950)\", \"user_rating\": 4.0},\n",
        "    {\"movie_title\": \"Aladdin and the King of Thieves (1996)\", \"user_rating\": 4.0},\n",
        "    {\"movie_title\": \"Dumbo (1941)\", \"user_rating\": 4.0}\n",
        "]\n",
        "\n",
        "new_user_preferences_2 = [\n",
        "    {\"movie_title\": \"Star Wars (1977)\", \"user_rating\": 5.0},\n",
        "    {\"movie_title\": \"Stargate (1994)\", \"user_rating\": 5.0},\n",
        "    {\"movie_title\": \"Robert A. Heinlein's The Puppet Masters (1994)\", \"user_rating\": 4.0},\n",
        "    {\"movie_title\": \"Jurassic Park (1993)\", \"user_rating\": 4.0},\n",
        "    {\"movie_title\": \"Twelve Monkeys (1995)\", \"user_rating\": 4.0},\n",
        "    {\"movie_title\": \"Terminator 2: Judgment Day (1991)\", \"user_rating\": 4.0}\n",
        "]\n",
        "\n",
        "recommendations_1 = recommend_movies(model, data, new_user_preferences_1, movie_titles)\n",
        "print(recommendations_1)\n",
        "recommendations_2 = recommend_movies(model, data, new_user_preferences_2, movie_titles)\n",
        "print(recommendations_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnrENSh5FVBe",
        "outputId": "528dff09-f7cd-4775-e66f-934ceee247b9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1563/1563 [==============================] - 11s 6ms/step - loss: 0.6109 - val_loss: -2.5699\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5827 - val_loss: -2.1237\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.5771 - val_loss: -2.3502\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5771 - val_loss: -2.5472\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5781 - val_loss: -2.4841\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.5797\n",
            "53/53 [==============================] - 0s 1ms/step\n",
            "['Twelve Monkeys (1995)' 'Seven (Se7en) (1995)'\n",
            " 'Muppet Treasure Island (1996)' 'Braveheart (1995)' 'I.Q. (1994)'\n",
            " 'Santa Clause, The (1994)' 'James and the Giant Peach (1996)'\n",
            " 'Philadelphia Story, The (1940)' 'Vertigo (1958)' 'M (1931)']\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.5791\n",
            "53/53 [==============================] - 0s 3ms/step\n",
            "['Seven (Se7en) (1995)' 'Ed Wood (1994)' 'I.Q. (1994)'\n",
            " 'Professional, The (1994)' 'Santa Clause, The (1994)'\n",
            " 'Monty Python and the Holy Grail (1974)' 'Client, The (1994)'\n",
            " 'Spy Hard (1996)' 'Vertigo (1958)' 'Some Like It Hot (1959)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('recommendation_model', save_format='tf')"
      ],
      "metadata": {
        "id": "tAnCn5SKKv0F"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r file.zip recommendation_model\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"file.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "DR6-0B1MLIpn",
        "outputId": "8cbe5075-5a2c-4956-d2ea-8bfe3daf6184"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: recommendation_model/ (stored 0%)\n",
            "  adding: recommendation_model/assets/ (stored 0%)\n",
            "  adding: recommendation_model/saved_model.pb (deflated 87%)\n",
            "  adding: recommendation_model/variables/ (stored 0%)\n",
            "  adding: recommendation_model/variables/variables.index (deflated 56%)\n",
            "  adding: recommendation_model/variables/variables.data-00000-of-00001 (deflated 8%)\n",
            "  adding: recommendation_model/fingerprint.pb (stored 0%)\n",
            "  adding: recommendation_model/keras_metadata.pb (deflated 81%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7c922c60-ee3c-4a03-98f9-d800ae9cb47c\", \"file.zip\", 1491236)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}